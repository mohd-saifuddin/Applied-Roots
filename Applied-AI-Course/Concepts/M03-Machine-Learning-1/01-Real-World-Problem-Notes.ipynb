{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef9cf27",
   "metadata": {},
   "source": [
    "__Amazon Fine Food Review Analysis__\n",
    "\n",
    "Source: https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "\n",
    "The Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.\n",
    "\n",
    "- Number of reviews: 568,454\n",
    "- Number of users: 256,059\n",
    "- Number of products: 74,258\n",
    "- Timespan: Oct 1999 - Oct 2012\n",
    "- Number of Attributes/Columns in data: 10\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. Id\n",
    "2. ProductId - unique identifier for the product\n",
    "3. UserId - unqiue identifier for the user\n",
    "4. ProfileName\n",
    "5. HelpfulnessNumerator - number of users who found the review helpful\n",
    "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "7. Score - rating between 1 and 5\n",
    "8. Time - timestamp for the review\n",
    "9. Summary - brief summary of the review\n",
    "10. Text - text of the review\n",
    "\n",
    "__Objective__: Given a review, determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2).\n",
    "\n",
    "Q) How to determine if a review is positive or negative?\n",
    "\n",
    "A) We could use the Score/Rating. A rating of 4 or 5 could be cosnidered a positive review. A review of 1 or 2 could be considered negative. A review of 3 is nuetral and ignored. This is an approximate and proxy way of determining the polarity (positivity/negativity) of a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7627b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5448ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cebacfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0bd9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use(style='seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd740f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8325a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    NAME\n",
    "FROM\n",
    "    sqlite_master\n",
    "WHERE\n",
    "    type = \"table\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb141206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reviews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name\n",
       "0  Reviews"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tables = pd.read_sql_query(sql=query, con=con)\n",
    "display(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b24979",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    Reviews\n",
    "WHERE\n",
    "    Score <> 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a15db469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_data = pd.read_sql_query(sql=query, con=con)\n",
    "display(f_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a232d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = lambda x: 0 if x < 3 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f572fa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(partition(2))\n",
    "print(partition(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b13d382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "3                     3                       3      0  1307923200   \n",
       "4                     0                       0      1  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_data['Score'] = f_data['Score'].map(partition)\n",
    "display(f_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb27301b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90b6d2",
   "metadata": {},
   "source": [
    "__Exploratory Data Analysis__\n",
    "\n",
    "Data Cleaning: Deduplication\n",
    "\n",
    "It is observed that the reviews data had many duplicate entries. Hence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "134d2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    Reviews\n",
    "WHERE\n",
    "    Score <> 3\n",
    "AND\n",
    "    UserId = \"AR5J8UI46CURR\"\n",
    "ORDER BY\n",
    "    ProductId\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7ad943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = pd.read_sql_query(sql=query, con=con)\n",
    "display(d.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34f023a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138688</th>\n",
       "      <td>150506</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2IW4PEEKO2R0U</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1194739200</td>\n",
       "      <td>Love the book, miss the hard cover version</td>\n",
       "      <td>I grew up reading these Sendak books, and watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138689</th>\n",
       "      <td>150507</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A1S4A3IQ2MU7V4</td>\n",
       "      <td>sally sue \"sally sue\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1191456000</td>\n",
       "      <td>chicken soup with rice months</td>\n",
       "      <td>This is a fun way for children to learn their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138690</th>\n",
       "      <td>150508</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AZGXZ2UUK6X</td>\n",
       "      <td>Catherine Hallberg \"(Kate)\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1076025600</td>\n",
       "      <td>a good swingy rhythm for reading aloud</td>\n",
       "      <td>This is a great little book to read aloud- it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138691</th>\n",
       "      <td>150509</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A3CMRKGE0P909G</td>\n",
       "      <td>Teresa</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1018396800</td>\n",
       "      <td>A great way to learn the months</td>\n",
       "      <td>This is a book of poetry about the months of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                  ProfileName  \\\n",
       "138706  150524  0006641040   ACITT7DI6IDDL              shari zychinski   \n",
       "138688  150506  0006641040  A2IW4PEEKO2R0U                        Tracy   \n",
       "138689  150507  0006641040  A1S4A3IQ2MU7V4        sally sue \"sally sue\"   \n",
       "138690  150508  0006641040     AZGXZ2UUK6X  Catherine Hallberg \"(Kate)\"   \n",
       "138691  150509  0006641040  A3CMRKGE0P909G                       Teresa   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "138706                     0                       0      1   939340800   \n",
       "138688                     1                       1      1  1194739200   \n",
       "138689                     1                       1      1  1191456000   \n",
       "138690                     1                       1      1  1076025600   \n",
       "138691                     3                       4      1  1018396800   \n",
       "\n",
       "                                           Summary  \\\n",
       "138706                   EVERY book is educational   \n",
       "138688  Love the book, miss the hard cover version   \n",
       "138689               chicken soup with rice months   \n",
       "138690      a good swingy rhythm for reading aloud   \n",
       "138691             A great way to learn the months   \n",
       "\n",
       "                                                     Text  \n",
       "138706  this witty little book makes my son laugh at l...  \n",
       "138688  I grew up reading these Sendak books, and watc...  \n",
       "138689  This is a fun way for children to learn their ...  \n",
       "138690  This is a great little book to read aloud- it ...  \n",
       "138691  This is a book of poetry about the months of t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_data = f_data.sort_values(by='ProductId')\n",
    "display(s_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc21739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138688</th>\n",
       "      <td>150506</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2IW4PEEKO2R0U</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1194739200</td>\n",
       "      <td>Love the book, miss the hard cover version</td>\n",
       "      <td>I grew up reading these Sendak books, and watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138689</th>\n",
       "      <td>150507</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A1S4A3IQ2MU7V4</td>\n",
       "      <td>sally sue \"sally sue\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1191456000</td>\n",
       "      <td>chicken soup with rice months</td>\n",
       "      <td>This is a fun way for children to learn their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138690</th>\n",
       "      <td>150508</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AZGXZ2UUK6X</td>\n",
       "      <td>Catherine Hallberg \"(Kate)\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1076025600</td>\n",
       "      <td>a good swingy rhythm for reading aloud</td>\n",
       "      <td>This is a great little book to read aloud- it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138691</th>\n",
       "      <td>150509</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A3CMRKGE0P909G</td>\n",
       "      <td>Teresa</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1018396800</td>\n",
       "      <td>A great way to learn the months</td>\n",
       "      <td>This is a book of poetry about the months of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                  ProfileName  \\\n",
       "138706  150524  0006641040   ACITT7DI6IDDL              shari zychinski   \n",
       "138688  150506  0006641040  A2IW4PEEKO2R0U                        Tracy   \n",
       "138689  150507  0006641040  A1S4A3IQ2MU7V4        sally sue \"sally sue\"   \n",
       "138690  150508  0006641040     AZGXZ2UUK6X  Catherine Hallberg \"(Kate)\"   \n",
       "138691  150509  0006641040  A3CMRKGE0P909G                       Teresa   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "138706                     0                       0      1   939340800   \n",
       "138688                     1                       1      1  1194739200   \n",
       "138689                     1                       1      1  1191456000   \n",
       "138690                     1                       1      1  1076025600   \n",
       "138691                     3                       4      1  1018396800   \n",
       "\n",
       "                                           Summary  \\\n",
       "138706                   EVERY book is educational   \n",
       "138688  Love the book, miss the hard cover version   \n",
       "138689               chicken soup with rice months   \n",
       "138690      a good swingy rhythm for reading aloud   \n",
       "138691             A great way to learn the months   \n",
       "\n",
       "                                                     Text  \n",
       "138706  this witty little book makes my son laugh at l...  \n",
       "138688  I grew up reading these Sendak books, and watc...  \n",
       "138689  This is a fun way for children to learn their ...  \n",
       "138690  This is a great little book to read aloud- it ...  \n",
       "138691  This is a book of poetry about the months of t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final = s_data.drop_duplicates(subset={\"UserId\", \"ProfileName\", \"Time\", \"Text\"}, keep='first')\n",
    "display(final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12f83049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cff1b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.25890143662969"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "percent_data_retained = (len(final) / len(f_data)) * 100\n",
    "display(percent_data_retained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8dcda8",
   "metadata": {},
   "source": [
    "It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0018c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    Reviews\n",
    "WHERE\n",
    "    Score <> 3\n",
    "AND\n",
    "    Id = 44737\n",
    "OR\n",
    "    Id = 64422\n",
    "ORDER BY\n",
    "    ProductId\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32b4ded5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       1      5  1224892800   \n",
       "1                     3                       2      4  1212883200   \n",
       "\n",
       "                                        Summary  \\\n",
       "0             Bought This for My Son at College   \n",
       "1  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                Text  \n",
       "0  My son loves spaghetti so I didn't hesitate or...  \n",
       "1  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = pd.read_sql_query(sql=query, con=con)\n",
    "display(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff1aeed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final[final['HelpfulnessNumerator'] <= final['HelpfulnessDenominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e7a7bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a735946b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    307061\n",
       "0     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final['Score'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f4439",
   "metadata": {},
   "source": [
    "__Why convert text to a vector?__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159435439-74945e55-2c46-4885-a489-72b9bada63f2.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159435951-98adfd3d-c868-4a14-8084-d0621e37d9be.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159436491-55c78522-bde0-44bb-91b0-84f7dadb6792.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159436794-7ab634cd-f87c-4705-b142-c412c779724a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159437260-bf9ddd9e-4d9c-46f1-990b-576506ff5557.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ba159",
   "metadata": {},
   "source": [
    "__Bag of Words (BoW)__\n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159440518-ee6131ec-cc7b-43d1-bffb-265a27f6d271.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159440784-d2a73590-357d-41b1-9aa6-021fb9b62808.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159441109-4ce9ae82-4809-43e8-8042-77e92016232d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159442485-2d61a544-3404-4672-a559-f7d59c009c83.png)\n",
    "\n",
    "In the above screenshot, a small correction - During the construction of BOW, 'is' was mentioned twice which is a typo. We consider only unique words.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159443348-d6bff985-f28c-451e-843c-7058069baad5.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159443810-5030e7e3-9d46-45e6-8586-9778f6f2498c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68cc5f",
   "metadata": {},
   "source": [
    "__Text Preprocessing: Stemming, Stop Word Removal, Tokenization, Lemmatization__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159462702-77d51465-87b8-4363-a64a-5a9f79f62f75.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159463664-66a73912-afaa-41b8-94f0-fb72431a104a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159464223-ff94f614-1e04-472b-b443-137fed5fdd53.png)\n",
    "\n",
    "Here, teacher misdefined lemmatization. Teacher actually defined tokenization.\n",
    "\n",
    "Lemmatization is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatization depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document.\n",
    "\n",
    "Stemming and lemmatization are methods used by search engines and chatbots to analyze the meaning behind a word. Stemming uses the stem of the word, while lemmatization uses the context in which the word is being used.\n",
    "\n",
    "Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
    "\n",
    "For instance:\n",
    "\n",
    "- The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "- The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    "- The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*ES5bt7IoInIq2YioQp2zcQ.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159464930-ceabb311-0e2f-455a-801c-271fac9c9adc.png)\n",
    "\n",
    "Bag of Words __does not take semantic meaning of words__ into consideration --> major drawback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7406089c",
   "metadata": {},
   "source": [
    "__Uni-gram, Bi-gram, N-grams__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159475295-d9f2550b-66a2-4916-82b7-61d5c01dc195.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159472146-18548934-bb93-4fbf-b2f5-25f92790d2f7.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159472721-01a3c581-056d-4b54-b4bf-fb5a7d9e229d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159473287-2e7aae3e-c558-411a-9256-de4364b0e125.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159473773-5d2ec65b-a841-4a6a-b7d6-a64363a6f2f5.png)\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Rome is not built in a day\n",
    "\n",
    "Here there are no repeated words. So\n",
    "\n",
    "- unigrams --> not, in, day, is, a, built, Rome\n",
    "- bigrams --> Rome is, is not, not built, built in, in a, a day\n",
    "- trigrams --> Rome is not, is not built, not built in, built in a, in a day\n",
    "\n",
    "Here there is no repetetion of words in the given sentence. Hence, number of unigrams > number of bigrams > number of trigrams.\n",
    "\n",
    "Example 2:\n",
    "\n",
    "horse is a horse of course of course accept it\n",
    "\n",
    "Here there are repeated words. So\n",
    "- unigrams --> horse, of, course, a, is, it, accept\n",
    "- bigrams --> horse is, is a, a horse, horse of, of course, course of, course accept, accept it\n",
    "- trigrams --> horse is a, is a horse, a horse of, horse of course, of course of, course of course, of course accept, course accept it\n",
    "\n",
    "Here there is repetetion of words in the given sentence. Hence, number of unigrams < number of bigrams < number of trigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969c1a4",
   "metadata": {},
   "source": [
    "__TF-IDF (Term Frequency - Inverse Document Frequency)__\n",
    "\n",
    "Link: https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3\n",
    "\n",
    "Term Frequency (tf): gives us the frequency of the word in each document in the corpus. It is the ratio of number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases. Each document has its own tf.\n",
    "\n",
    "Inverse Data Frequency (idf): used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159481310-f2d176a1-8241-4538-aaba-139cc5a3f173.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159482073-d4873997-0a87-4ca2-931c-bc152a63f8f7.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159482859-bb799073-6bfa-47b8-ac5f-70dec49b68f1.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159483467-34770bf7-c4ac-4a26-ab77-70702f434cef.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159484864-2037a88d-5314-4822-b5d9-26fc700a8abf.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159484174-ee59058d-2b17-4b90-9d8f-96222f32f225.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159485169-cd78b1fa-c2c1-41b9-afde-e6ea6842ccfb.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159485536-9c99f90d-7097-4487-9122-59a46b120310.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159486081-90d557fe-7fac-4c4e-a8ad-422e1799ecaa.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159486475-007d311d-e8eb-4f9d-81f6-87bdd52c902f.png)\n",
    "\n",
    "TF-IDF still does not take semantic meaning of words. For example: tasty - delicious; cheap - affordable etc.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159489489-bc5af23d-09e8-4ee1-b1e0-4cacb3da46a8.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159490430-bfce1004-3f2d-4d84-9c1f-2d1c1c6a9adf.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159495275-816140f9-0402-4f5c-b1ed-d125243484a6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb493e4",
   "metadata": {},
   "source": [
    "__Word2Vec__\n",
    "\n",
    "Link: https://www.tensorflow.org/tutorials/text/word2vec\n",
    "\n",
    "Unlike Bag of Words and TF-IDF, Word2Vec considers semantic meaning of words.\n",
    "\n",
    "Word2vec is a technique for natural language processing published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159693183-e25caf01-c10f-459c-a023-151f95e24ff2.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159693835-2cbca91a-ea89-4e11-be96-1692346e2f49.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159694812-ea8b1117-06b3-41ec-96f0-0bdb757ff486.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159695216-b2654439-53a3-4187-b23f-71b58cb4aad4.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159695590-d31b59fb-87d5-4a11-8327-00861e4827d0.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159695912-fd92eaa4-d9b5-4ff7-b519-d154423c3351.png)\n",
    "\n",
    "In Summery, The idea behind Word2Vec is pretty simple. Were making an assumption that the meaning of a word can be inferred by the company it keeps. This is analogous to the saying, show me your friends, and Ill tell who you are\".\n",
    "\n",
    "If you have two words that have very similar neighbors (meaning: the context in which its used is about the same), then these words are probably quite similar in meaning or are at least related. For example, the words shocked, appalled, and astonished are usually used in a similar context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b7d76",
   "metadata": {},
   "source": [
    "__Avg-Word2Vec, tf-idf weighted Word2Vec__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159698672-a8bf4ca7-ff7c-40d7-8d62-e4ca695b2e9f.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159699203-ae3e88c9-74cc-4202-afb1-832ebb1c8840.png)\n",
    "\n",
    "For each word, a vector is created. In Avg W2V, as each word has a vector associated with it, average of all the vectors (component-wise) of all the words in a given review is computed.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159699697-2213f30c-8534-4d25-a4db-aee2b402acca.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159700087-50e95ea9-57b9-40d1-b27f-86669c15d415.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159700450-7b22f21f-6f2e-4264-899d-5def27690c7f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fcc7be",
   "metadata": {},
   "source": [
    "Sparse Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d778a",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/160226121-15b02e04-d186-4dd4-aadb-a5978f45dd01.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/160226175-cc3e90f9-6258-403a-bed0-0d532ed5471b.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/160226220-2c0f7ac3-f910-4e34-9aaf-0892d043ce40.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/160226257-8a9efd10-077c-46c6-b6f1-b355cd601326.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/160226388-b9f0b1df-2c49-4e74-b481-a0aee09bebe1.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/160226414-75743709-8705-4f04-b845-a8fea4803252.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/160226456-47042a88-a60b-4e3e-a84e-af0a17b99975.png)\n",
    "\n",
    "Correction: sparsity of the matrix = no.of zero elements / total elements\n",
    "\n",
    "A matrix is sparse if many of its coefficients are zero. The interest in sparsity arises because its exploitation can lead to enormous computational savings and because many large matrix problems that occur in practice are sparse.\n",
    "By contrast, if most of the elements are nonzero, then the matrix is considered dense.\n",
    "\n",
    "sparsity = count zero elements / total elements\n",
    "\n",
    "Density = 1 - sparsity\n",
    "\n",
    "Dense matrices store every entry in the matrix. Sparse matrices only store the nonzero entries. Sparse matrices don't have a lot of extra features, and some algorithms may not work for them.\n",
    "You use them when you need to work with matrices that would be too big for the computer to handle them, but they are mostly zero, so they compress easily.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/160226504-870e51f5-131e-4583-9228-5212157772e7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0565f",
   "metadata": {},
   "source": [
    "Text Preprocessing\n",
    "\n",
    "Now that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n",
    "\n",
    "Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "- Begin by removing the html tags.\n",
    "- Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "- Check if the word is made up of english letters and is not alpha-numeric.\n",
    "- Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters).\n",
    "- Convert the word to lowercase.\n",
    "- Remove Stopwords.\n",
    "- Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming).\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1b9dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73423005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1a4f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52855112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 364171/364171 [01:06<00:00, 5478.24it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_reviews = []\n",
    "\n",
    "for sentance in tqdm(final['Text'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z0-9]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_reviews.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6817d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "late delivered paid extra charge faster delivery not delivered time expected\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_reviews[5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8b4fe",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b179ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3b737",
   "metadata": {},
   "source": [
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using `scipy.sparse.csr_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a323914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "final_counts = count_vec.fit_transform(raw_documents=preprocessed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de7dbe7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(364171, 116756)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(type(final_counts))\n",
    "display(final_counts.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40552e2",
   "metadata": {},
   "source": [
    "There are $116756$ unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6cb4cd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<364171x116756 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11971404 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82609aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 114149)\t1\n",
      "  (0, 59223)\t1\n",
      "  (0, 11737)\t3\n",
      "  (0, 61190)\t1\n",
      "  (0, 94893)\t2\n",
      "  (0, 57622)\t1\n",
      "  (0, 60044)\t1\n",
      "  (0, 83783)\t2\n",
      "  (0, 15503)\t1\n",
      "  (0, 30987)\t1\n",
      "  (0, 2903)\t1\n",
      "  (0, 3161)\t1\n",
      "  (0, 92822)\t1\n",
      "  (0, 84409)\t1\n",
      "  (0, 57892)\t1\n",
      "  (0, 112927)\t1\n",
      "  (0, 50921)\t1\n",
      "  (0, 31032)\t1\n",
      "  (0, 87454)\t1\n",
      "  (0, 60092)\t1\n",
      "  (0, 68406)\t1\n",
      "  (0, 114517)\t1\n",
      "  (0, 52446)\t1\n",
      "  (0, 92606)\t1\n",
      "  (0, 19480)\t1\n",
      "  :\t:\n",
      "  (364170, 42138)\t1\n",
      "  (364170, 55927)\t1\n",
      "  (364170, 60097)\t1\n",
      "  (364170, 81602)\t1\n",
      "  (364170, 48258)\t1\n",
      "  (364170, 34742)\t1\n",
      "  (364170, 86701)\t1\n",
      "  (364170, 102632)\t1\n",
      "  (364170, 66612)\t1\n",
      "  (364170, 111837)\t1\n",
      "  (364170, 7241)\t1\n",
      "  (364170, 4777)\t1\n",
      "  (364170, 94047)\t1\n",
      "  (364170, 87359)\t1\n",
      "  (364170, 14730)\t1\n",
      "  (364170, 91809)\t1\n",
      "  (364170, 36623)\t1\n",
      "  (364170, 46244)\t1\n",
      "  (364170, 14774)\t1\n",
      "  (364170, 14742)\t1\n",
      "  (364170, 90693)\t2\n",
      "  (364170, 27063)\t1\n",
      "  (364170, 88140)\t1\n",
      "  (364170, 30417)\t1\n",
      "  (364170, 64175)\t1\n"
     ]
    }
   ],
   "source": [
    "print(final_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60328cc",
   "metadata": {},
   "source": [
    "Bi-Grams and N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2079843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(ngram_range=(1, 2))\n",
    "final_bigram_counts = count_vec.fit_transform(raw_documents=preprocessed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d42b4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(364171, 3923364)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(type(final_bigram_counts))\n",
    "display(final_bigram_counts.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "baae40c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<364171x3923364 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 25563547 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f526f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3851370)\t1\n",
      "  (0, 1950745)\t1\n",
      "  (0, 359218)\t3\n",
      "  (0, 2040306)\t1\n",
      "  (0, 3188176)\t2\n",
      "  (0, 1869528)\t1\n",
      "  (0, 1991302)\t1\n",
      "  (0, 2797693)\t2\n",
      "  (0, 502657)\t1\n",
      "  (0, 1025765)\t1\n",
      "  (0, 82455)\t1\n",
      "  (0, 99161)\t1\n",
      "  (0, 3109089)\t1\n",
      "  (0, 2813835)\t1\n",
      "  (0, 1879762)\t1\n",
      "  (0, 3818823)\t1\n",
      "  (0, 1711593)\t1\n",
      "  (0, 1026631)\t1\n",
      "  (0, 2912578)\t1\n",
      "  (0, 1992116)\t1\n",
      "  (0, 2252623)\t1\n",
      "  (0, 3860812)\t1\n",
      "  (0, 1753801)\t1\n",
      "  (0, 3097651)\t1\n",
      "  (0, 630236)\t1\n",
      "  :\t:\n",
      "  (364170, 904231)\t1\n",
      "  (364170, 477276)\t1\n",
      "  (364170, 1000292)\t1\n",
      "  (364170, 1000353)\t1\n",
      "  (364170, 2715235)\t1\n",
      "  (364170, 1819265)\t1\n",
      "  (364170, 3155585)\t1\n",
      "  (364170, 2202584)\t1\n",
      "  (364170, 3029214)\t1\n",
      "  (364170, 200451)\t1\n",
      "  (364170, 3188276)\t1\n",
      "  (364170, 132576)\t1\n",
      "  (364170, 3765893)\t1\n",
      "  (364170, 2890217)\t1\n",
      "  (364170, 474355)\t1\n",
      "  (364170, 3073885)\t1\n",
      "  (364170, 2125026)\t1\n",
      "  (364170, 670325)\t1\n",
      "  (364170, 2909507)\t1\n",
      "  (364170, 3481904)\t1\n",
      "  (364170, 2928746)\t1\n",
      "  (364170, 1142206)\t1\n",
      "  (364170, 477980)\t1\n",
      "  (364170, 1424915)\t1\n",
      "  (364170, 2125028)\t1\n"
     ]
    }
   ],
   "source": [
    "print(final_bigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d410d",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0265fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ef7c0",
   "metadata": {},
   "source": [
    "Convert a collection of raw documents to a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1d1b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer(ngram_range=(1, 2))\n",
    "final_tf_idf = tf_idf_vec.fit_transform(raw_documents=preprocessed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70ed8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_features = tf_idf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8cc92244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(364171, 3923364)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3923364"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(type(final_tf_idf))\n",
    "display(final_tf_idf.get_shape())\n",
    "display(len(tf_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5587578d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['always enjoyable',\n",
       " 'always enjoyed',\n",
       " 'always enjoyedthe',\n",
       " 'always enjoying',\n",
       " 'always enjoys',\n",
       " 'always enough',\n",
       " 'always enoughk',\n",
       " 'always ensues',\n",
       " 'always ensure',\n",
       " 'always ensures']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tf_features[100000:100010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "466ca615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "3923364\n"
     ]
    }
   ],
   "source": [
    "print(final_tf_idf[3, :].toarray()[0])\n",
    "print(len(final_tf_idf[3, :].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ce94ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tfidf_features(row, features, top_n=25):\n",
    "    \"\"\"\n",
    "    This funtion gets the top 'n' tfidf features.\n",
    "    \"\"\"\n",
    "    topn_ids = np.argsort(a=row)[::-1][:top_n]\n",
    "    top_feats = [[features[i], row[i]] for i in topn_ids]\n",
    "    df = pd.DataFrame(data=top_feats, columns=['feature', 'tfidf'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "140b2c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recite</td>\n",
       "      <td>0.264566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>0.219890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>introduces silliness</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loud recite</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>memory college</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>india drooping</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>roses love</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>whales india</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>classic book</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>always sing</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>recite memory</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>silliness classic</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>able recite</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>book willing</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>words book</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>son laugh</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>recite car</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sing refrain</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>refrain learned</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>learned whales</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bet son</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>book introduces</td>\n",
       "      <td>0.146257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>witty little</td>\n",
       "      <td>0.141734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>laugh loud</td>\n",
       "      <td>0.141734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>along always</td>\n",
       "      <td>0.141734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature     tfidf\n",
       "0                 recite  0.264566\n",
       "1                   book  0.219890\n",
       "2   introduces silliness  0.146257\n",
       "3            loud recite  0.146257\n",
       "4         memory college  0.146257\n",
       "5         india drooping  0.146257\n",
       "6             roses love  0.146257\n",
       "7           whales india  0.146257\n",
       "8           classic book  0.146257\n",
       "9            always sing  0.146257\n",
       "10         recite memory  0.146257\n",
       "11     silliness classic  0.146257\n",
       "12           able recite  0.146257\n",
       "13          book willing  0.146257\n",
       "14            words book  0.146257\n",
       "15             son laugh  0.146257\n",
       "16            recite car  0.146257\n",
       "17          sing refrain  0.146257\n",
       "18       refrain learned  0.146257\n",
       "19        learned whales  0.146257\n",
       "20               bet son  0.146257\n",
       "21       book introduces  0.146257\n",
       "22          witty little  0.141734\n",
       "23            laugh loud  0.141734\n",
       "24          along always  0.141734"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_tfidf_df = get_top_tfidf_features(row=final_tf_idf[0, :].toarray()[0], features=tf_features)\n",
    "display(top_tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ca4c0",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/160231764-d605bda1-2eba-48df-84e8-9123d0de34b0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea75b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
